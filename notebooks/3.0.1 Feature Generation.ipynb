{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn this notebook, we generate features and come up with the models\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "In this notebook, we generate features and come up with the models\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for array manipulations\n",
    "import numpy as np\n",
    "#for image processing\n",
    "import cv2 \n",
    "#for displaying images\n",
    "import matplotlib.pyplot as plt\n",
    "#to display images in this notebook, not in a separate window\n",
    "%matplotlib inline\n",
    "#to access system resources such as directories\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set this to point to the project root; all paths will be relative to this one\n",
    "project_dir = '/home/lyle/tutorials/AI/scikit-learn/maize-disease-detection/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_directories(project_dir=project_dir):\n",
    "    \"\"\"Sets up the paths to important direcoties\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    project_dir : string; default is the current working directory\n",
    "        The path to the project root i.e '/home/lyle/tutorials/AI/scikit-learn/maize-disease-detection/'\n",
    "    \n",
    "    returns\n",
    "    -------\n",
    "    base_dir : string\n",
    "        The project directory path\n",
    "    data_folder : string\n",
    "        The data subfolder path\n",
    "    maize_data_folder : \n",
    "        The path to the subdirectory containing the maize images\n",
    "        \n",
    "    example usage\n",
    "    -------------\n",
    "    base_dir, data_folder, maize_data_folder = set_up_directories()\n",
    "    \"\"\"\n",
    "    \n",
    "    #set our base directory. This should point to the location of the plant-diseases folder\n",
    "    base_dir = project_dir\n",
    "    #set the path to our data folder\n",
    "    data_folder = os.path.join(base_dir, 'data')\n",
    "    #set the path to the maize folder and list the various categories available\n",
    "    maize_data_folder = os.path.join(data_folder, 'maize')\n",
    "\n",
    "    return base_dir, data_folder, maize_data_folder\n",
    "\n",
    "def get_32(disease):\n",
    "    \"\"\"Loads 32 images for a given maize disease\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    disease: string\n",
    "        A string that could be common_rust, healthy, leaf_spot, nothern_leaf_blight\n",
    "    returns\n",
    "    -------\n",
    "    disease_images: list\n",
    "        A list of images for the selected disease\n",
    "    \"\"\"\n",
    "    \n",
    "    #this list will contain the 20 images returned\n",
    "    disease_images = []\n",
    "    #path to the images\n",
    "    disease_images_path = os.path.join(maize_data_folder, disease)\n",
    "    for image_path in os.listdir(disease_images_path):\n",
    "        image_path = os.path.join(disease_images_path, image_path)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        disease_images.append(image)\n",
    "    return disease_images\n",
    "\n",
    "#This function will help us plot 10 images\n",
    "def plot_images(images, title):\n",
    "    \"\"\"Plots 10 images of a particular disease category\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    images: list\n",
    "        List of images(each image is an array)\n",
    "    title: string\n",
    "        Title for each image i.e name of disease\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(12,8))\n",
    "    for i in range(10):\n",
    "        plt.subplot(2,5, i+1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(title)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "#This function allows us to resize images\n",
    "def resize(image, new_size=(600,600)):\n",
    "    \"\"\"Resize the given image\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    image : numpy array\n",
    "        The image to be resized\n",
    "    new_size : tuple\n",
    "        The new image size\n",
    "    returns\n",
    "    -------\n",
    "    resized_image : numpy arra\n",
    "        The resized image\n",
    "    \"\"\"\n",
    "    \n",
    "    resized_image = cv2.resize(image, new_size)\n",
    "    return resized_image\n",
    "\n",
    "#This function generates ORB features\n",
    "def extract_features_orb(image, vector_size=32):\n",
    "    \"\"\"Extracts orb features for the given image\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    image : numpy array\n",
    "        The image whose features are to be extracted\n",
    "    vector_size : int\n",
    "        The number of keypoints to use\n",
    "    returns\n",
    "    -------\n",
    "        orb_decriptors : \n",
    "        \n",
    "    raises\n",
    "    ------\n",
    "    cv2.error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        feature_generator = cv2.ORB_create()\n",
    "        orb_keypoints = feature_generator.detect(image)\n",
    "        orb_keypoints = orb_keypoints[:32]\n",
    "        orb_keypoints, orb_descriptors = feature_generator.compute(image, orb_keypoints)\n",
    "        orb_descriptors = orb_descriptors.flatten()\n",
    "        #The descriptor vector size is 128\n",
    "        needed_size = (vector_size*128)\n",
    "        if orb_descriptors.size < needed_size:\n",
    "            #If we have less than 32 keypoints, add zeros to the end of our vector\n",
    "            orb_descriptors = np.concatenate([orb_descriptors, np.zeros(needed_size - orb_descriptors.size)])\n",
    "    except cv2.error as e:\n",
    "        print(f'Error: {e}')\n",
    "        return None\n",
    "    return orb_descriptors\n",
    "\n",
    "#This function generates KAZE features\n",
    "def extract_features_kaze(image, vector_size=32):\n",
    "    \"\"\"Extracts kaze features for the given image\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    image : numpy array\n",
    "        The image whose features are to be extracted\n",
    "    vector_size : int\n",
    "        The number of keypoints to use\n",
    "    returns\n",
    "    -------\n",
    "        kaze_descriptors : \n",
    "        \n",
    "    raises\n",
    "    ------\n",
    "    cv2.error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        feature_generator = cv2.KAZE_create()\n",
    "        kaze_keypoints = feature_generator.detect(image)\n",
    "        kaze_keypoints = kaze_keypoints[:32]\n",
    "        kaze_keypoints, kaze_descriptors = feature_generator.compute(image, kaze_keypoints)\n",
    "        kaze_descriptors = kaze_descriptors.flatten()\n",
    "        #The descriptor vector size is 128\n",
    "        needed_size = (vector_size*128)\n",
    "        if kaze_descriptors.size < needed_size:\n",
    "            #If we have less than 32 keypoints, add zeros to the end of our vector\n",
    "            kaze_descriptors = np.concatenate([kaze_descriptors, np.zeros(needed_size - kaze_descriptors.size)])\n",
    "    except cv2.error as e:\n",
    "        print(f'Error: {e}')\n",
    "        return None\n",
    "    return kaze_descriptors\n",
    "\n",
    "def extract_hog_features(image, feature_size=4096):\n",
    "    \"\"\"Extracts hog features for the image\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    image : numpy array\n",
    "        The image whose features are to be extracted\n",
    "    feature_size : int\n",
    "        The number of features to generate\n",
    "    returns\n",
    "    -------\n",
    "        hog_features : numpy array \n",
    "        \n",
    "    raises\n",
    "    ------\n",
    "    cv2.error\n",
    "    \"\"\"\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    features = hog.compute(common_rust_images[0])\n",
    "    required_features = features[:feature_size].ravel()\n",
    "    return required_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directories set up\n",
    "base_dir, data_folder, maize_data_folder = set_up_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_rust_images = get_32('common_rust')\n",
    "healthy_images = get_32('healthy')\n",
    "leaf_spot_images = get_32('leaf_spot')\n",
    "nothern_leaf_blight_images = get_32('nothern_leaf_blight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this example, we generate a sample dataset, using the sample kaze features we just generated\n",
    "#A dataset usually consists of an items features and the corresponding labels\n",
    "#This will be our feature set\n",
    "sample_kaze_features =[]\n",
    "#This will contain our labels\n",
    "sample_kaze_labels = []\n",
    "sample_kaze_features.append(common_rust_image_kaze_features)\n",
    "sample_kaze_labels.append('common_rust')\n",
    "sample_kaze_features.append(healthy_image_kaze_features)\n",
    "sample_kaze_labels.append('healthy')\n",
    "sample_kaze_features.append(leaf_spot_image_kaze_features)\n",
    "sample_kaze_labels.append('leaf_spot')\n",
    "sample_kaze_features.append(nothern_leaf_blight_image_kaze_features)\n",
    "sample_kaze_labels.append('nothern_leaf_blight')\n",
    "sample_kaze_labels = np.array(sample_kaze_labels)\n",
    "sample_kaze_features = np.array(sample_kaze_features)\n",
    "#In our dataset, we have four instances and each instance has 4096 features\n",
    "sample_kaze_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have four labels, each label just tells us the disease type\n",
    "sample_kaze_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, we create a sample dataset for the model. X and y are the standard variables used\n",
    "X = sample_kaze_features\n",
    "y = sample_kaze_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Scaling is one of the preprocessing steps to enable our models work better\n",
    "X = StandardScaler().fit_transform(X)\n",
    "#We split into a train set and a test set\n",
    "X_train, X_test = train_test_split(X, test_size=0.1, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Our train data set is of shape {X_train.shape}')\n",
    "print(f'Our test dataset is of shape {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#We encode our labels into numerical values since that is the kind of dataset that models work with\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "#We then split into a train and test set\n",
    "y_train, y_test = train_test_split(y, test_size=0.1, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let us perform these steps for all the 32 images loaded\n",
    "#This will contain all our images\n",
    "all_images = []\n",
    "#This will contain all our labels\n",
    "all_labels = []\n",
    "labels = ['common_rust', 'healthy', 'leaf_spot', 'nothern_leaf_blight']\n",
    "for i, image_folder in enumerate([common_rust_images, healthy_images, leaf_spot_images, nothern_leaf_blight_images]):\n",
    "    for image in image_folder:\n",
    "        all_images.append(image)\n",
    "        all_labels.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_images), len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us extraxt KAZE features\n",
    "def extract_features():\n",
    "    features, labels = [], []\n",
    "    for i, image in enumerate(all_images):\n",
    "        image_features = extract_features_kaze(image)\n",
    "        image_label = all_labels[i]\n",
    "        features.append(image_features)\n",
    "        labels.append(image_label)\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    features = StandardScaler().fit_transform(features)\n",
    "    labels = LabelEncoder().fit_transform(labels)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now these two, y_train and X_train can be fed into any classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
