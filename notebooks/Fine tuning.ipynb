{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data augmentation, noise addition\n",
    "Descriptors\n",
    "Model perfomance\n",
    "Fine tuning\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for array manipulations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#for image processing\n",
    "import cv2 \n",
    "#for displaying images\n",
    "import matplotlib.pyplot as plt\n",
    "#to display images in this notebook, not in a separate window\n",
    "%matplotlib inline\n",
    "#to access system resources such as directories\n",
    "import os\n",
    "#This wilallow us to get the training time of each model\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, auc, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\Documents\\\\GitHub\\\\maize-disease-detection\\\\notebooks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set our base directory. This should point to the location of the plant-diseases folder\n",
    "base_dir = 'C:\\\\Users\\\\USER\\\\Documents\\\\GitHub\\\\maize-disease-detection'\n",
    "data_folder = os.path.join(base_dir, 'data')\n",
    "maize_data_folder = os.path.join(data_folder, 'maize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function loads 32 images of a particular disease\n",
    "def get_32(disease):\n",
    "    '''\n",
    "    disease:\n",
    "        A string that could be common_rust, healthy, leaf_spot, nothern_leaf_blight\n",
    "    ........\n",
    "    disease_images:\n",
    "        A list of images for the selected disease\n",
    "    '''\n",
    "    #this list will contain the 20 images returned\n",
    "    disease_images = []\n",
    "    #path to the images\n",
    "    disease_images_path = os.path.join(maize_data_folder, disease)\n",
    "    for image_path in os.listdir(disease_images_path):\n",
    "        image_path = os.path.join(disease_images_path, image_path)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        disease_images.append(image)\n",
    "    return disease_images\n",
    "\n",
    "#This function will help us plot 10 images\n",
    "def plot_images(images, title):\n",
    "    '''\n",
    "    images: List\n",
    "        List of images\n",
    "    title: String\n",
    "        Title for each image i.e name of disease\n",
    "    '''\n",
    "    plt.figure(figsize=(15,6))\n",
    "    for i in range(10):\n",
    "        plt.subplot(2,5, i+1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(title)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "#This function generates ORB features\n",
    "def extract_features_orb(image, vector_size=32):\n",
    "    try:\n",
    "        feature_generator = cv2.ORB_create()\n",
    "        orb_keypoints = feature_generator.detect(image)\n",
    "        orb_keypoints = orb_keypoints[:32]\n",
    "        orb_keypoints, orb_descriptors = feature_generator.compute(image, orb_keypoints)\n",
    "        orb_descriptors = orb_descriptors.flatten()\n",
    "        #The descriptor vector size is 128\n",
    "        needed_size = (vector_size*128)\n",
    "        if orb_descriptors.size < needed_size:\n",
    "            #If we have less than 32 keypoints, add zeros to the end of our vector\n",
    "            orb_descriptors = np.concatenate([orb_descriptors, np.zeros(needed_size - orb_descriptors.size)])\n",
    "    except cv2.error as e:\n",
    "        print(f'Error: {e}')\n",
    "        return None\n",
    "    return orb_descriptors\n",
    "\n",
    "#This function generates KAZE features\n",
    "def extract_features_kaze(image, vector_size=32):\n",
    "    try:\n",
    "        feature_generator = cv2.KAZE_create()\n",
    "        kaze_keypoints = feature_generator.detect(image)\n",
    "        kaze_keypoints = kaze_keypoints[:32]\n",
    "        kaze_keypoints, kaze_descriptors = feature_generator.compute(image, kaze_keypoints)\n",
    "        kaze_descriptors = kaze_descriptors.flatten()\n",
    "        #The descriptor vector size is 128\n",
    "        needed_size = (vector_size*128)\n",
    "        if kaze_descriptors.size < needed_size:\n",
    "            #If we have less than 32 keypoints, add zeros to the end of our vector\n",
    "            kaze_descriptors = np.concatenate([kaze_descriptors, np.zeros(needed_size - kaze_descriptors.size)])\n",
    "    except cv2.error as e:\n",
    "        print(f'Error: {e}')\n",
    "        return None\n",
    "    return kaze_descriptors\n",
    "\n",
    "def extract_features_hog(image, feature_size=4096):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    features = hog.compute(image)\n",
    "    required_features = features[:feature_size].ravel()\n",
    "    return required_features\n",
    "\n",
    "#Let us extraxt KAZE features\n",
    "def extract_features(algorithm=0):\n",
    "    '''\n",
    "    Algorithm:\n",
    "        1 for ORB\n",
    "        0 for KAZE\n",
    "        2 for HOG\n",
    "    '''\n",
    "    #Now let us perform these steps for all the 32 images loaded\n",
    "    #This will contain all our images\n",
    "    all_images = []\n",
    "    #This will contain all our labels\n",
    "    all_labels = []\n",
    "    labels = ['common_rust', 'healthy', 'leaf_spot', 'nothern_leaf_blight']\n",
    "    for i, image_folder in enumerate([common_rust_images, healthy_images, leaf_spot_images, nothern_leaf_blight_images]):\n",
    "        for image in image_folder:\n",
    "            all_images.append(image)\n",
    "            all_labels.append(labels[i])\n",
    "    features, labels = [], []\n",
    "    for i, image in enumerate(all_images):\n",
    "        image_features = []\n",
    "        try:\n",
    "            if algorithm == 1:\n",
    "                image_features = extract_features_orb(image)\n",
    "            elif algorithm == 0:\n",
    "                image_features = extract_features_kaze(image)\n",
    "            else:\n",
    "                image_features = extract_features_hog(image)\n",
    "            image_label = all_labels[i]\n",
    "            features.append(image_features)\n",
    "            labels.append(image_label)\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    features = StandardScaler().fit_transform(features)\n",
    "    labels = LabelEncoder().fit_transform(labels)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#Let us extraxt KAZE features\n",
    "def extract_features2(algorithm=0):\n",
    "    '''\n",
    "    Algorithm:\n",
    "        1 for ORB\n",
    "        0 for KAZE\n",
    "        2 for HOG\n",
    "    '''\n",
    "    #Now let us perform these steps for all the 32 images loaded\n",
    "    #This will contain all our images\n",
    "    all_images = []\n",
    "    #This will contain all our labels\n",
    "    all_labels = []\n",
    "    labels = ['common_rust', 'healthy', 'leaf_spot', 'nothern_leaf_blight']\n",
    "    for i, image_folder in enumerate([common_rust_images, healthy_images, leaf_spot_images, nothern_leaf_blight_images]):\n",
    "        for image in image_folder:\n",
    "            all_images.append(image)\n",
    "            all_labels.append(labels[i])\n",
    "    features, labels = [], []\n",
    "    for i, image in enumerate(all_images):\n",
    "        image_features = []\n",
    "        try:\n",
    "            if algorithm == 1:\n",
    "                image_features = extract_features_orb(image)\n",
    "            elif algorithm == 0:\n",
    "                image_features = extract_features_kaze(image)\n",
    "            else:\n",
    "                image_features = extract_features_hog(image)\n",
    "            image_label = all_labels[i]\n",
    "            features.append(image_features)\n",
    "            labels.append(image_label)\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    features = StandardScaler().fit_transform(features)\n",
    "    labels = LabelEncoder().fit_transform(labels)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "#This method gives us a rough idea about the accuracy of the base models and their raining times\n",
    "def train_base_models(X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    '''\n",
    "    model_accuracy = []\n",
    "    train_time = []\n",
    "    model_names = []\n",
    "    for i, classifier in enumerate(models):\n",
    "        try:\n",
    "            #Let us train the model and get the training time\n",
    "            start_time = time.time()\n",
    "            classifier.fit(X_train, y_train)\n",
    "            stop_time = time.time()\n",
    "            train_time.append(stop_time - start_time)\n",
    "            predictions = classifier.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, predictions)\n",
    "            model_accuracy.append(round(accuracy, 3))\n",
    "            model_names.append(names[i])\n",
    "            print(f'{names[i]}: {round(accuracy, 3)}')\n",
    "        except Exception as e:\n",
    "            print(f'Could not train {names[i]} because of {e}')\n",
    "    df = pd.DataFrame({'Model':model_names, 'Accuracy':model_accuracy, 'Train Time':train_time})\n",
    "    df = df.sort_values(by=['Accuracy'], ascending=False)\n",
    "    return df\n",
    "\n",
    "#This method gives us a rough idea about the accuracy of the base models and their raining times\n",
    "def train_base_models2(features, labels):\n",
    "    '''\n",
    "    '''\n",
    "    #We use cross-validation\n",
    "    model_accuracy = []\n",
    "    train_time = []\n",
    "    model_names = []\n",
    "    \n",
    "    for i, classifier in enumerate(models):\n",
    "        try:\n",
    "            #Let us train the model and get the training time\n",
    "            start_time = time.time()\n",
    "            scores = cross_val_score(classifier, features, labels, scoring='accuracy', cv=10)\n",
    "            stop_time = time.time()\n",
    "            train_time.append(stop_time - start_time)\n",
    "            model_accuracy.append(round(np.mean(scores), 3))\n",
    "            model_names.append(names[i])\n",
    "            print(f'{names[i]}: {round(np.mean(scores), 3)}')\n",
    "        except Exception as e:\n",
    "            print(f'Could not train {names[i]} because of {e}')\n",
    "    df = pd.DataFrame({'Model':model_names, 'Accuracy':model_accuracy, 'Train Time':train_time})\n",
    "    df = df.sort_values(by=['Accuracy'], ascending=False)\n",
    "    return df\n",
    "\n",
    "#This function returns the model perfomance on kaze data\n",
    "def kaze_results(algorithm=0):\n",
    "    models_names = []\n",
    "    model_perfomance = []\n",
    "    train_time = []\n",
    "    for i, model in enumerate(kaze_models):\n",
    "        start = time.time()\n",
    "        scores = cross_val_score(model, X,y, scoring='accuracy', cv=10)\n",
    "        stop = time.time()\n",
    "        models_names.append(model_names[i])\n",
    "        model_perfomance.append(round(scores.mean(), 2))\n",
    "        train_time.append((stop - start)/10)\n",
    "    df = pd.DataFrame({'Model Name':models_names, 'Model Accuracy':model_perfomance, 'Training time':train_time})\n",
    "    df = df.sort_values(by=['Model Accuracy'], ascending=False)\n",
    "    return df\n",
    "\n",
    "#This function returns the model perfomance on hog data\n",
    "def hog_results(X,y):\n",
    "    models_names = []\n",
    "    model_perfomance = []\n",
    "    train_time = []\n",
    "    for i, model in enumerate(hog_models):\n",
    "        start = time.time()\n",
    "        scores = cross_val_score(model, X, y, scoring='accuracy', cv=10)\n",
    "        stop = time.time()\n",
    "        models_names.append(model_names[i])\n",
    "        model_perfomance.append(round(scores.mean(), 2))\n",
    "        train_time.append((stop - start)/10)\n",
    "    df = pd.DataFrame({'Model Name':models_names, 'Model Accuracy':model_perfomance, 'Training Time':train_time})\n",
    "    df = df.sort_values(by=['Model Accuracy', 'Training Time'], ascending=False)\n",
    "    return df\n",
    "\n",
    "#This method generates new features for our training\n",
    "def get_new_features(model, features):\n",
    "    model_rfecv = RFECV(estimator=model, step=1, cv=kfold, scoring='accuracy')\n",
    "    model_results = model_rfecv.fit(features, labels) \n",
    "    model_grid_scores = model_rfecv.grid_scores_\n",
    "    model_ranking = knn_rfecv.ranking_\n",
    "    model_features_support = model_rfecv.support_\n",
    "    model_new_features = []\n",
    "    for i,feature in enumerate(model_features_support):\n",
    "        if feature:\n",
    "            model_new_features.append(i)\n",
    "    model_features = features[:,knn_new_features]\n",
    "    return model_features\n",
    "\n",
    "#This method trains a modelon the new features\n",
    "def train_model(model, features, labels):\n",
    "    start = time.time()\n",
    "    scores = cross_val_score(model, features, labels, scoring='accuracy', cv=10)\n",
    "    stop = time.time()\n",
    "    t = (stop - start)/10\n",
    "    return round(scores.mean(), 2), round(t, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
